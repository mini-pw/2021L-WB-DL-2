{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intended-being",
   "metadata": {},
   "source": [
    "# Praca domowa nr 3\n",
    "\n",
    "Adrianna Grudzień\\\n",
    "29 marca 2021 r. \n",
    "\n",
    "W ramach pracy domowej z przedmiotu Warsztaty badawcze zapoznanałam się z pojęciem `Ablation study`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-decade",
   "metadata": {},
   "source": [
    "# `Ablation study` - badanie ablacji\n",
    "\n",
    "Badanie ablacji bada wydajność systemu sztucznej inteligencji poprzez usunięcie niektórych komponentów, aby zrozumieć wkład komponentu w cały system. Termin ten jest przez analogię do biologii (usuwanie składników organizmu) i kontynuując analogię, jest szczególnie używany w analizie sztucznych sieci neuronowych, analogicznie do ablacyjnej chirurgii mózgu. Badania ablacji wymagają, aby system wykazywał wdzięczną degradację: aby nadal funkcjonował, nawet gdy brakuje pewnych składników lub są one zdegradowane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-court",
   "metadata": {},
   "source": [
    "# Przykład"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "advanced-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import models, layers\n",
    "from keras.datasets import imdb\n",
    "from sklearn.metrics import classification_report, precision_score\n",
    "\n",
    "\n",
    "MAX_NUM_WORDS = 20\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=MAX_NUM_WORDS)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "# Funkcja potrzebna, żeby w każdej recenzji było tyle samo liczb\n",
    "# Tam, gdzie jest ich mniej niż MAX_NUM_WORDS, uzupełniamy zerami\n",
    "# W pozostałych przypadkach zliczamy liczbę użytych słów.\n",
    "# ~ coś w rodzaju one hot encoding\n",
    "def wektoryzacja(data, rozmiar=MAX_NUM_WORDS):\n",
    "    output = np.zeros((len(data), rozmiar))\n",
    "    for i, seq in enumerate(data):\n",
    "        for j in seq:\n",
    "            output[i,j] += 1\n",
    "    return output\n",
    "# Wektoryzacja \n",
    "def wektoryzacja_2(data, rozmiar=MAX_NUM_WORDS):\n",
    "    output = np.zeros((len(data), rozmiar))\n",
    "    for i, seq in enumerate(data):\n",
    "        output[i,seq] = 1\n",
    "    return output\n",
    "\n",
    "data = wektoryzacja(data)\n",
    "targets = wektoryzacja_2(targets,2)\n",
    "targets = np.array(targets).astype('float32')\n",
    "\n",
    "X_train = data[MAX_NUM_WORDS:]\n",
    "y_train = targets[MAX_NUM_WORDS:]\n",
    "X_test = data[:MAX_NUM_WORDS]\n",
    "y_test = targets[:MAX_NUM_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "maritime-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Możemy przyjrzeć się zmiennej docelowej:\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "coated-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wykres(results):\n",
    "#     plt.figure(figsize=(15,5))\n",
    "#     # summarize history for accuracy\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(results.history['accuracy'])\n",
    "#     plt.plot(results.history['val_accuracy'])\n",
    "#     plt.title('Model accuracy')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['acc ~ train', 'val_acc ~ test'], loc='upper left')\n",
    "#     # summarize history for loss\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(results.history['loss'])\n",
    "#     plt.plot(results.history['val_loss'])\n",
    "#     plt.title('Model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "def czas_treningu(X_test, y_test, model):\n",
    "    start = time.time()\n",
    "    results = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "    end = time.time()\n",
    "    print(\"Czas treningu: \", end-start, 's')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "nuclear-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja do opracowania wyniku:\n",
    "def analiza_modelu(X_test, y_test, model):\n",
    "    results = czas_treningu(X_test, y_test, model)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('loss: ',score[0])\n",
    "    print('accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-tomorrow",
   "metadata": {},
   "source": [
    "### Budowa modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "extensive-leeds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_307 (Dense)            (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,252\n",
      "Trainable params: 6,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Czas treningu:  20.478740692138672 s\n",
      "loss:  0.6511842012405396\n",
      "accuracy:  0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# Input layer\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(MAX_NUM_WORDS, )))\n",
    "# Hidden layers:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.5, noise_shape=None, seed=None))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# kompilujemy model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# trenujemy model\n",
    "analiza_modelu(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-border",
   "metadata": {},
   "source": [
    "# Dodawanie/odejmowanie `warstw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "upset-financing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu:  139.21098279953003 s\n",
      "loss:  0.6209396123886108\n",
      "accuracy:  0.550000011920929\n"
     ]
    }
   ],
   "source": [
    "# dodanie warstw\n",
    "\n",
    "model = models.Sequential()\n",
    "# Input layer\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(MAX_NUM_WORDS, )))\n",
    "# Hidden layers:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "# Output layer\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# kompilujemy model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "analiza_modelu(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "atmospheric-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu:  95.57809782028198 s\n",
      "loss:  0.5147997736930847\n",
      "accuracy:  0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "# usunięcie warstw Dropout\n",
    "\n",
    "model = models.Sequential()\n",
    "# Input layer\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(MAX_NUM_WORDS, )))\n",
    "# Hidden layers:\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "analiza_modelu(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-sacrifice",
   "metadata": {},
   "source": [
    "Wychodzi na to, że w przypadku tych danych, im więcej warstw, tym accuracy gorsze i im mniej warstw, tym accuracy większe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-stuff",
   "metadata": {},
   "source": [
    "# Zmiana parametrów `warstw`\n",
    "\n",
    "Zamiast softmax jest sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "alone-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu:  164.43071794509888 s\n",
      "loss:  0.6727690100669861\n",
      "accuracy:  0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# Input layer\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(MAX_NUM_WORDS, )))\n",
    "# Hidden layers:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.5, noise_shape=None, seed=None))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# kompilujemy model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# trenujemy model\n",
    "analiza_modelu(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-bolivia",
   "metadata": {},
   "source": [
    "Model trenuje się dłużej, loss jest większy, ale accuracy nie zmienia się."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-diversity",
   "metadata": {},
   "source": [
    "### W warstwach Dense: zamiast 50 - 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "trying-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu:  196.11490750312805 s\n",
      "loss:  0.5822020173072815\n",
      "accuracy:  0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# Input layer\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(MAX_NUM_WORDS, )))\n",
    "# Hidden layers:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dropout(0.5, noise_shape=None, seed=None))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# kompilujemy model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# trenujemy model\n",
    "analiza_modelu(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-legislature",
   "metadata": {},
   "source": [
    "Model trenuje się ponad dwa razy dłużej, loss jest mniejszy, accuracy - bez zmian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-lightweight",
   "metadata": {},
   "source": [
    "### Zmiana parametrów `treningu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "caring-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu:  246.3105251789093 s\n",
      "loss:  0.6067681312561035\n",
      "accuracy:  0.6499999761581421\n",
      "Czas treningu:  211.8729703426361 s\n",
      "loss:  0.6000439524650574\n",
      "accuracy:  0.6499999761581421\n",
      "Czas treningu:  211.44419050216675 s\n",
      "loss:  0.5919212102890015\n",
      "accuracy:  0.6000000238418579\n",
      "Czas treningu:  197.89110898971558 s\n",
      "loss:  0.6036732196807861\n",
      "accuracy:  0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "odel = models.Sequential()\n",
    "# Input layer\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(MAX_NUM_WORDS, )))\n",
    "# Hidden layers:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.5, noise_shape=None, seed=None))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "analiza_modelu(X_test, y_test, model)\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "analiza_modelu(X_test, y_test, model)\n",
    "\n",
    "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "analiza_modelu(X_test, y_test, model)\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "analiza_modelu(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-signal",
   "metadata": {},
   "source": [
    "Jak widać, dla różnych parametrów dostajemy różne wyniki. Jeśli w tym przypadku zależy nam na czasie, to najlepiej będzie wybrać optimizer='sgd', ale nie zawsze będzie to odpowiednie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
