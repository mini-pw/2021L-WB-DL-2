{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation study - czym jest?\n",
    "Zgodnie ze artykułem na wikipedii : https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)  \n",
    "Ablation study jest zajmuje się badaniem wydajności systemów sztucznej inteligencji poprzez usuwanie niektórych części systemu by móc lepiej zrozumieć jego rolę, zaangażowanie w całość. Badanie zdegradowanych systemów do momentu całkowitego zaprzestania ich funkcjonalności, niesie ze sobą informację o wytrzymałości, wrażliwości oraz złożoności systemu.  \n",
    "Tak jak wedle artykuły na wikipedii, pojęcie występuje w biologii i tyczy się odpowiednio równoważnego pojęcia badania zdeprawowanych z pewnych systemów organizmów i obserwacji ich funkcjonalności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie ramki danych oraz sieci neuronowej\n",
    "W moim Ablation study operować będę na zbiorze CIFAR 10, oraz na autorsko stworzonej sieci neuronowej.  \n",
    "Źródło do zbioru danych CIFAR-10 : https://www.cs.toronto.edu/~kriz/cifar.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Załadujmy potrzebne pakiety\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train), (X_test,y_test) = datasets.cifar10.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zanim przejdziemy dalej, muszę dorzucić kilka słów a propos naszej ramki danych. Jak wiemy, CIFAR-10 zawiera 60000 grafik rbg z 10 klasami, w postaci enumeracyjnej z wartościami:  \n",
    "0 - airplane  \n",
    "1 - automoblie  \n",
    "2 - bird  \n",
    "3 - cat  \n",
    "4 - deer  \n",
    "5 - dog  \n",
    "6 - frog  \n",
    "7 - horse  \n",
    "8 - ship  \n",
    "9 - truck  \n",
    "Jednakże zerknijmy teraz na nasze y_train oraz y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]] \n",
      "\n",
      "[[3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5], \"\\n\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy jest to dwuwymiarowy wektor, jednakże taki obiekt jest całkowicie zbędny, więc zastąpmy go po prostu jedno wymiarowym. Następnie będziemy mogli także dokonać łatwiej enkodowania zmiennych, np. onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 4 1] \n",
      "\n",
      "[3 8 8 0 6]\n"
     ]
    }
   ],
   "source": [
    "#Teraz lepiej\n",
    "print(y_train[:5], \"\\n\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla optymalizacji zeskalujmy teraz dane w zbiorach testowym oraz treningowym oraz zastosujmy enkodowanie dla zmiennych numerycznych w y_test oraz y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test /255\n",
    "X_train = X_train /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie modelu\n",
    "Ok, początek mamy z głowy, teraz przejdźmy do ciekawszej części zadania, czyli do utworzenia modelu sieci neuronowej, w celach strzeszczeniowych dany model nie będzie przesadnie rozbudowany, lecz będzie spełniał wymogi zadania. Dla niektórych zmiennych w warstwach sieci neuronowej, ich wartość jest równa ich wartości domyślnej, oznacza to, iż będę eksperymentował z tymi danymi i należało to podkreślić w tworzeniu modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = keras.Sequential(\n",
    "[\n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, zanim przejdziemy do ablation study, sprawdźmy poprawność powyższego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                262208    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 282,250\n",
      "Trainable params: 282,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(original_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 1.6325 - accuracy: 0.4107\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.0426 - accuracy: 0.6361\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 0.9206 - accuracy: 0.6787\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.8239 - accuracy: 0.7149\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.7421 - accuracy: 0.7408\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6808 - accuracy: 0.7640\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6254 - accuracy: 0.7804\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.5604 - accuracy: 0.8037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f34e5a3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "original_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.9398 - accuracy: 0.6895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9398031234741211, 0.6894999742507935]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sprawdźmy jak sprawuje się model na danych testowych\n",
    "original_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzyskalismy około 73% accuracy, co dla 4 epok jest nie najgorszym wynikiem. Ograniczam się wyłącznie do 4 epok, ze względu na czasochłonnosć mojego modelu oraz względnie dobrego wyniku accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Właściwe albation study - dodawanie/usuwanie warstw\n",
    "Przejdźmy wreszcie do właściwej części naszego zadania, czyli do eksperymentowania z naszymi parametrami. Na początku utwórzmy pewne inne, zmodyfikowane wersje naszej oryginalnej sieci neuronowej.  \n",
    "Zacznijmy z usuwaniem warstw, gdyż, wedle mnie, łatwo pokaże nam to, które warstwy naszej sieci są najbardziej newralgiczne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W oryginalnym modelu nie uwzględniłem warstyw dropout chroniącej przed przeuczaniem, sprawdźmy czy dodanie ów, zmieni wynik\n",
    "mod_1_model = keras.Sequential(\n",
    "[  \n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 1.7520 - accuracy: 0.3656\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 1.2470 - accuracy: 0.5554\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 1.1073 - accuracy: 0.6104\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 1.0273 - accuracy: 0.6399\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.9586 - accuracy: 0.6620\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.9128 - accuracy: 0.67790s - loss: 0.9128 - accuracy: 0.\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.8760 - accuracy: 0.6902\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8356 - accuracy: 0.7076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3d39c3c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_1_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_1_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 1.3924 - accuracy: 0.5328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3924410343170166, 0.532800018787384]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Podobny wynik dla zbioru testowego\n",
    "mod_1_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważyliśmy pwien spadek accuracy, zatem nasz model potrzebuje większej ilości danych by nauczyć się lepiej, oraz wyłączenie  15% neuronów przy wejściu obniżył jakość modelu o 10 punktów procentowych, również wejście powłoki Dropout usuytuowane na początku powinna \"na logikę\" zmieniać wagai pomiędzy neuronami w największym stopniu. O ironio, zabieg, który w zamiarze miał przeciwdziałać przeuczeniu, na zbiorze testowym wykazał się gorszą accuracy :(.    \n",
    "Spróbujmy jeszcze raz z warstwą dropout, tym razem przy samym końcu naszych warstw, czy nasza \"logika\" sprawdzi się i accuracy będzie większe?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_2_model = keras.Sequential(\n",
    "[  \n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 1.7202 - accuracy: 0.3719\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 1.1587 - accuracy: 0.5905\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9953 - accuracy: 0.6488\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9259 - accuracy: 0.6728\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8608 - accuracy: 0.6949\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.8005 - accuracy: 0.71850s - loss: 0.8005 - accu\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.7388 - accuracy: 0.7392\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.7074 - accuracy: 0.7471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3c111048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_2_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_2_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 1.0071 - accuracy: 0.6700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0070972442626953, 0.6700000166893005]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Podobny wynik dla zbioru testowego\n",
    "mod_2_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, jest lepiej niż poprzednio, co zakładałem uprzednio. Zanim przejdziemy do innego aspektu naszych powłok, dokonajmy testu dla warstwy dropout **przed** warstwą Flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_3_model = keras.Sequential(\n",
    "[  \n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    layers.Dropout(0.15),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 1.6673 - accuracy: 0.3961\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 1.1082 - accuracy: 0.6127\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.9536 - accuracy: 0.6678\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.8753 - accuracy: 0.6981\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 0.7952 - accuracy: 0.7224\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.7394 - accuracy: 0.7418\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.6972 - accuracy: 0.7561\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.6553 - accuracy: 0.7718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3ebd0390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_3_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_3_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.8906 - accuracy: 0.7001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8906192183494568, 0.7001000046730042]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Podobny wynik dla zbioru testowego\n",
    "mod_3_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efekt jest praktcznie identyczny, choć lepszy, również accuracy dla zbiory testowego jest również lepsze, czyli dropout spełnił swoje zadanie :D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dobrze dodawaliśmy, teraz czas zabrać. Stosowaliśmy dwie warstwy konwolucyjne oraz dwie warstwy MaxPoolingu, jednakże jak wiemy z opisu zbioru danych CIFAR-10, zamieszczone tam grafiki są nieznacznych rozdzielczości, więc zastosowanie podwójnej konwolucji może okazać się zbyt dużym przedsięwzięciem. Zatem zabierzmy jedną z nich, wraz z MaxPooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_4_model = keras.Sequential(\n",
    "[  \n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.7901 - accuracy: 0.3578\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2856 - accuracy: 0.5475\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1591 - accuracy: 0.5916\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0793 - accuracy: 0.6160\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.0259 - accuracy: 0.6387\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.9748 - accuracy: 0.65650s - l\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9409 - accuracy: 0.66650s - loss: 0.9408 - accuracy: \n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9161 - accuracy: 0.6746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3f01b668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_4_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_4_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 1.1131 - accuracy: 0.6169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1130871772766113, 0.6169000267982483]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_4_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zabranie jednej warstwy konwolucyjnej wrawz z MaxPoolingiem zmniejszyło, choć znieznacznie, accuracy, jednakże wnioskować możemy, iż zwiększanie ilości warst konwolucyjnych nie będzie poprawiało wyników w nieskończnoność, w szczególności dla niewielkich grafik jak w naszym przypadku.  \n",
    "Miałem w planach przeprowadzenie testu modelu z jeszcze jedną dodatkową warstwą konwolucyjną, lecz jakość mojego komputera nie pozwoliłaby mi na to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametry w warstwach\n",
    "W następnym kroku zabawmy się parametrami w istniejących już warstwach.  \n",
    "Dla przypomnienia, nasz model prezentuje się następująco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                262208    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 282,250\n",
      "Trainable params: 282,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zacznijmy pierw od modyfikacji funkcji aktywującej dla wsztskich warstw, za wyjątkiem ostatniej. Relu jest dość popularną oraz sprawdzoną metodą jeżeli chodzi o funkcje aktywacji. Zatem przetestujmy coś bardziej \"egzotycznego\". Zobaczmy co się stanie dla funkcji exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_5_model = keras.Sequential(\n",
    "[\n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'exponential'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'exponential'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'exponential'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 53s 33ms/step - loss: 752.5754 - accuracy: 0.0984\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 2.3029 - accuracy: 0.0962\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 2.3028 - accuracy: 0.1024\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 2.3189 - accuracy: 0.1022\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 2.3168 - accuracy: 0.0960\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 54s 34ms/step - loss: 2.3008 - accuracy: 0.1048\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 2.2298 - accuracy: 0.1511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3f3c62b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_5_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_5_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 2.2894 - accuracy: 0.1028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.289391279220581, 0.10279999673366547]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_5_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest, celny strzał zatapiający naszą siec neuronową, accuracy naszej sieci zostało zdegradowane do ledwie 15%, lecz cóż od tego jest **Ablation study**.  No dobrze, spróbujmy jeszcze raz z funkcją aktywacji, tym razem z funkcją sigmoid, czym bardziej sprawdzonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_6_model = keras.Sequential(\n",
    "[\n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'sigmoid'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'sigmoid'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 2.3166 - accuracy: 0.0968\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 2.3027 - accuracy: 0.1020\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 2.3027 - accuracy: 0.1021\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 2.3028 - accuracy: 0.1003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3f529390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_6_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_6_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 2.3027 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.302654981613159, 0.10000000149011612]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_6_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kolejny model zatopiony, ok może zmieńmy cel naszych parametrów, w tym przypadku jednak dodatkowo czas wykonywania był znacząco dłuższy, funkcja sigmoid zabierała dużo czasu na kompilacje. Wiemy, że nasze dane są obrazami o rozdzielczości 32x32. W oryginalnym modelu użyliśmy dość niewielkiej wielkości pool'u (2,2) oraz użyliśmy paddingu = 'same', zatem warstwa konwolucji nie zmieniała rozmiarów grafiki. Zmieńmy padding na domyślny 'valid' oraz zwiększmy rozmiar pool'u do (4,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_7_model = keras.Sequential(\n",
    "[\n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"valid\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(4, 4)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"valid\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(4, 4)),\n",
    "  \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.9123 - accuracy: 0.2858\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4452 - accuracy: 0.4813\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3248 - accuracy: 0.5266\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2346 - accuracy: 0.5642\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.1733 - accuracy: 0.5870\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.1318 - accuracy: 0.6007\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.0905 - accuracy: 0.6159\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0610 - accuracy: 0.6280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1603396c2e8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_7_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "mod_7_model.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 1.1421 - accuracy: 0.6038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1420811414718628, 0.6037999987602234]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_7_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy odrobinę niższe, lecz ostatecznie wyniki nie były najgorsze, niemniej jednak precyzyjniejsza analiza okazała się lepsza nawet dla niewielkich grafik, choć przy większym poolingu jest znacznie szybsza!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametry treningu\n",
    "Po uprzednich zniszczeniach spowodowanych naruszaniem homeostazy idealnych warunków modelu, teraz przyjrzyjmy się samemu treningowi oraz potencjalnym zmianom dla nauki modelu.  \n",
    "Zacznijmy od sprawdzenia innego optimiziea, mianowicie dość popularnego SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_prim = keras.Sequential(\n",
    "[\n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    layers.Conv2D(32,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64,kernel_size = (3,3),padding=\"same\",strides=(1,1),activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(10,activation = 'softmax'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 10.4820 - accuracy: 0.0966\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 10.4790 - accuracy: 0.0944\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 10.5130 - accuracy: 0.0968\n",
      "Epoch 4/8\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 10.4438 - accuracy: 0.1034 0s - loss: 10.4436 - accura\n",
      "Epoch 5/8\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 10.4781 - accuracy: 0.1003\n",
      "Epoch 6/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 10.4473 - accuracy: 0.0990\n",
      "Epoch 7/8\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 10.4729 - accuracy: 0.0978 0s - loss: 10.4732 - acc\n",
      "Epoch 8/8\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 10.5092 - accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15f3f39ee48>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_prim.compile(optimizer = 'SGD', loss = 'poisson',metrics=['accuracy'])\n",
    "\n",
    "original_model_prim.fit(X_train,y_train,epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 10.4617 - accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.461690902709961, 0.09799999743700027]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_prim.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer oraz loss function mają newralgiczne znaczenie przy efektywności sieci neuronowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak zobaczyliśmy powyżej **Ablation** study pozwolił nam na całkowite zdeprawowanie modelu, ale jak i również na znalezienie kilku równie dobrze działających alternatyw, choć niestety nie było żadnej lepszej pod względem accuracy.  \n",
    "Wartym wypunktowania jest fakt, iż nawet niewieka wartość w modelu jak funkcja aktywacji może zadecydować o całkowitej funkcjonalności lub niefunkcjonalności sieci neuronowej, co pokazuje jak złożone i wrażliwe na zmiany są.  \n",
    "Oczywiście nie jestem w stanie przeprowadzić pewnego odpowiednika \"strojenia parametrów\" z dużą precyzją jak w problemach machine learningu, gdyż złożoność sieci neuronowych przekracza możliwości mojego (i nie tylko mojego) komputera. Przy ogromnej różnorodności parametrów i ich wartości, takie zadanie okazuje się niemożliwe dla pojedyńczej jednostki.  \n",
    "Niemniej jednak ablation study, tak jak jej odpowiednik w biologii, pokazuje jak pewne parametry pojedyńczo wpływają na całość."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
